{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gauravksingh16/automated-multiple-eye-disease-detection/blob/main/MEDD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vTDDvODH_Kq5"
      },
      "outputs": [],
      "source": [
        "!rm -rf sample_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2c_UQLeo50en"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ODeBCaBS60-y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e34959c4-8756-40b3-a6ce-05ace8712ec6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BVZ-TXdtSIhI"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "abMCT25YLvOA"
      },
      "outputs": [],
      "source": [
        "!pip install lime\n",
        "!pip install timm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-HHVek9TM7yc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from typing import List, Tuple\n",
        "from PIL import ImageFile\n",
        "from google.colab.patches import cv2_imshow\n",
        "import csv\n",
        "from torchvision import transforms\n",
        "import torch\n",
        "from skimage.segmentation import mark_boundaries\n",
        "import lime\n",
        "from lime import lime_image\n",
        "import matplotlib.pyplot as plt\n",
        "import timm\n",
        "from functools import partial\n",
        "import torch.nn as nn\n",
        "import timm.models.vision_transformer\n",
        "from timm.models.layers import trunc_normal_\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
        "from torchvision import transforms\n",
        "import torchvision.models as models\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "import seaborn as sns\n",
        "import torch.cuda.amp as amp\n",
        "from torch.cuda.amp import GradScaler, autocast"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uNuPT8Ecv3eH"
      },
      "outputs": [],
      "source": [
        "# !rm -rf /content/drive/MyDrive/EyeDataset/Preprocessed Images/\n",
        "# !rm -rf /content/drive/MyDrive/EyeDataset/crop_info.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JY4Ywq_OdRc0"
      },
      "source": [
        "# **Image Preprocessing Steps**\n",
        "**Reading and Writing Images:** These functions, imread and imwrite, provide convenient wrappers for reading and writing image files using OpenCV in Python. The imread function reads an image from a file path, decoding it with optional color specification. It ensures a valid image and converts it to RGB format if necessary. The imwrite function writes an image to a specified file path, handling color conversions and ensuring non-empty images. Both functions enhance ease of use for image file operations in Python.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "**Folder Handling:** It helps in managing folders by creating them if they don't exist.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "**Mask Generation:** The \"get_mask_BZ\" function processes an input image to create a binary mask. It converts a color image to grayscale, sets a threshold for binary masking, and refines the mask by filling holes and applying morphological operations. The final mask accurately delineates object boundaries and is suitable for image analysis or computer vision applications.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Circle Detection:** These functions, _get_center_by_edge and _get_radius_by_mask_center, operate on binary masks of objects.\n",
        "\n",
        "\"_get_center_by_edge\" calculates the center coordinates of an object based on the edge information in the mask. It identifies the vertical and horizontal centers by finding positions where the sum of pixel values is above 95% of the maximum, providing a list of coordinates.\n",
        "\n",
        "\"_get_radius_by_mask_center\" determines the radius of an object given its binary mask and center coordinates. It employs morphological operations to enhance edges, calculates the Euclidean distance of edge points to the center, and derives the radius by identifying the most frequent distance. If no valid distances are found, a default radius of 0 is assigned. Both functions contribute to object analysis in image processing.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "**Mask and Image Manipulation:** \"The _get_circle_by_center_bbox\" function generates a binary circular mask based on specified center coordinates, a bounding box, and radius. The \"get_mask\" function processes an input image to obtain a circular mask, along with information about the bounding box, center coordinates, and radius. The \"mask_image\" function applies a binary mask to an input image, setting non-masked pixels to zero. Additionally, the \"remove_back_area\" function removes the background area from an image based on either a specified bounding box or a custom border. The \"supplemental_black_area\" function adds supplemental black areas to an image to match a specified border. Overall, these functions provide essential tools for manipulating and analyzing images, particularly in the context of object detection and segmentation.\n",
        "\n",
        "---\n",
        "\n",
        "The \"process_without_gb\" function takes an input image, label, and lists for storing radius, center coordinates, and border information. It processes the input image by extracting a mask, removing background areas, and handling supplemental black areas. The function returns the processed image, border information, a mask converted to unsigned 8-bit integers, the processed label, and updated lists for radius and center coordinates.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8oVieLPlSZGt"
      },
      "outputs": [],
      "source": [
        "def imread(file_path: str, c: int = None) -> np.ndarray:\n",
        "    if c is None:\n",
        "        im = cv2.imdecode(np.fromfile(file_path, dtype=np.uint8), cv2.IMREAD_UNCHANGED)\n",
        "    else:\n",
        "        im = cv2.imdecode(np.fromfile(file_path, dtype=np.uint8), c)\n",
        "\n",
        "    if im is None:\n",
        "        raise Exception('Cannot read image')\n",
        "\n",
        "    if im.ndim == 3 and im.shape[2] == 3:\n",
        "        im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n",
        "    return im\n",
        "\n",
        "def imwrite(file_path: str, image: np.ndarray) -> None:\n",
        "    if image is None or image.size == 0:\n",
        "        print(\"Image is empty or None. Skipping image write.\")\n",
        "        return\n",
        "\n",
        "    if image.ndim == 3 and image.shape[2] == 3:\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
        "    elif image.dtype == np.bool_:\n",
        "        image = (image * 255).astype(np.uint8)\n",
        "\n",
        "    cv2.imwrite(file_path, image)\n",
        "\n",
        "def fold_dir(folder: str) -> str:\n",
        "    if not os.path.exists(folder):\n",
        "        os.makedirs(folder)\n",
        "    return folder\n",
        "\n",
        "def get_mask_BZ(img: np.ndarray) -> np.ndarray:\n",
        "    if img.ndim == 3:\n",
        "        gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    else:\n",
        "        gray_img = img\n",
        "    threshold = np.mean(gray_img) / 3 - 5\n",
        "    _, mask = cv2.threshold(gray_img, max(5, threshold), 1, cv2.THRESH_BINARY)\n",
        "\n",
        "    nn_mask = np.zeros((mask.shape[0] + 2, mask.shape[1] + 2), np.uint8)\n",
        "    new_mask = (1 - mask).astype(np.uint8)\n",
        "    _, new_mask, _, _ = cv2.floodFill(new_mask, nn_mask, (0, 0), (0), cv2.FLOODFILL_MASK_ONLY)\n",
        "    _, new_mask, _, _ = cv2.floodFill(new_mask, nn_mask, (new_mask.shape[1] - 1, new_mask.shape[0] - 1), (0), cv2.FLOODFILL_MASK_ONLY)\n",
        "    mask = mask + new_mask\n",
        "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (20,  20))\n",
        "    mask = cv2.erode(mask, kernel)\n",
        "    mask = cv2.dilate(mask, kernel)\n",
        "    return mask\n",
        "\n",
        "def _get_center_by_edge(mask: np.ndarray) -> List[int]:\n",
        "    center = [0, 0]\n",
        "    x = mask.sum(axis=1)\n",
        "    center[0] = np.where(x > x.max() * 0.95)[0].mean()\n",
        "    x = mask.sum(axis=0)\n",
        "    center[1] = np.where(x > x.max() * 0.95)[0].mean()\n",
        "    return center\n",
        "\n",
        "def _get_radius_by_mask_center(mask: np.ndarray, center: List[int]) -> int:\n",
        "    mask = mask.astype(np.uint8)\n",
        "    ksize = max(mask.shape[1] // 400 * 2 + 1, 3)\n",
        "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (ksize, ksize))\n",
        "    mask = cv2.morphologyEx(mask, cv2.MORPH_GRADIENT, kernel)\n",
        "\n",
        "    index = np.where(mask > 0)\n",
        "    d_int = np.sqrt((index[0] - center[0]) ** 2 + (index[1] - center[1]) ** 2)\n",
        "    b_count = np.bincount(np.ceil(d_int).astype(int))\n",
        "    # Check if b_count is empty\n",
        "    if not b_count.any():\n",
        "        radius = 0  # Set a default radius\n",
        "    else:\n",
        "        radius = int(np.where(b_count > b_count.max() * 0.995)[0].max())\n",
        "    return radius\n",
        "\n",
        "    return radius\n",
        "\n",
        "def _get_circle_by_center_bbox(shape: Tuple[int, int], center: List[int], bbox: Tuple[int, int, int, int], radius: int) -> np.ndarray:\n",
        "    center_mask = np.zeros(shape=shape).astype('uint8')\n",
        "    tmp_mask = np.zeros(shape=bbox[2:4])\n",
        "    center_tmp = (int(center[0]), int(center[1]))\n",
        "    center_mask = cv2.circle(center_mask, center_tmp[::-1], int(radius), (1), -1)\n",
        "    return center_mask\n",
        "\n",
        "def get_mask(img: np.ndarray) -> Tuple[np.ndarray, Tuple[int, int, int, int], List[int], int]:\n",
        "    if img.ndim == 3:\n",
        "        g_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "    elif img.ndim == 2:\n",
        "        g_img = img.copy()\n",
        "    else:\n",
        "        raise Exception('Image dimension is not 1 or 3')\n",
        "\n",
        "    h, w = g_img.shape\n",
        "    shape = g_img.shape[0:2]\n",
        "    tg_img = cv2.normalize(g_img, None, 0, 255, cv2.NORM_MINMAX)\n",
        "    tmp_mask = get_mask_BZ(tg_img)\n",
        "    center = _get_center_by_edge(tmp_mask)\n",
        "    radius = _get_radius_by_mask_center(tmp_mask, center)\n",
        "\n",
        "    center = [center[0], center[1]]\n",
        "    radius = int(radius)\n",
        "    s_h = max(0, int(center[0] - radius))\n",
        "    s_w = max(0, int(center[1] - radius))\n",
        "    bbox = (s_h, s_w, min(h - s_h, 2 * radius), min(w - s_w, 2 * radius))\n",
        "    tmp_mask = _get_circle_by_center_bbox(shape, center, bbox, radius)\n",
        "    return tmp_mask, bbox, center, radius\n",
        "\n",
        "def mask_image(img: np.ndarray, mask: np.ndarray) -> np.ndarray:\n",
        "    img[mask <= 0, ...] = 0\n",
        "    return img\n",
        "\n",
        "def remove_back_area(img: np.ndarray, bbox: Tuple[int, int, int, int] = None, border: np.ndarray = None) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    image = img\n",
        "    if border is None:\n",
        "        border = np.array((bbox[0], bbox[0] + bbox[2], bbox[1], bbox[1] + bbox[3], img.shape[0], img.shape[1]), dtype=int)\n",
        "    image = image[border[0]:border[1], border[2]:border[3], ...]\n",
        "    return image, border\n",
        "\n",
        "def supplemental_black_area(img: np.ndarray, border: np.ndarray = None) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    image = img\n",
        "    if border is None:\n",
        "        h, v = img.shape[0:2]\n",
        "        max_l = max(h, v)\n",
        "        if image.ndim > 2:\n",
        "            image = np.zeros(shape=[max_l, max_l, img.shape[2]], dtype=int)\n",
        "        else:\n",
        "            image = np.zeros(shape=[max_l, max_l], dtype=int)\n",
        "        border = (int(max_l / 2 - h / 2), int(max_l / 2 - h / 2) + h, int(max_l / 2 - v / 2), int(max_l / 2 - v / 2) + v, max_l)\n",
        "    else:\n",
        "        max_l = border[4]\n",
        "        if image.ndim > 2:\n",
        "            image = np.zeros(shape=[max_l, max_l, img.shape[2]], dtype=img.dtype)\n",
        "        else:\n",
        "            image = np.zeros(shape=[max_l, max_l], dtype=img.dtype)\n",
        "    image[border[0]:border[1], border[2]:border[3], ...] = img\n",
        "    return image, border\n",
        "\n",
        "def process_without_gb(img: np.ndarray, label: np.ndarray, radius_list: List[int],\n",
        "                       centre_list_w: List[int], centre_list_h: List[int]) -> Tuple[np.ndarray, List, np.ndarray, np.ndarray, List[int], List[int], List[int]]:\n",
        "    borders = []\n",
        "    mask, bbox, center, radius = get_mask(img)\n",
        "    r_img = mask_image(img, mask)\n",
        "    r_img, r_border = remove_back_area(r_img, bbox=bbox)\n",
        "    mask, _ = remove_back_area(mask, border=r_border)\n",
        "    label, _ = remove_back_area(label, bbox=bbox)\n",
        "    borders.append(r_border)\n",
        "    r_img, sup_border = supplemental_black_area(r_img)\n",
        "    label, sup_border = supplemental_black_area(label)\n",
        "    mask, _ = supplemental_black_area(mask, border=sup_border)\n",
        "    borders.append(sup_border)\n",
        "\n",
        "    radius_list.append(radius)\n",
        "    centre_list_w.append(int(center[0]))\n",
        "    centre_list_h.append(int(center[1]))\n",
        "\n",
        "    return r_img, borders, (mask * 255).astype(np.uint8), label, radius_list, centre_list_w, centre_list_h"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOB4uTtvdoDn"
      },
      "source": [
        "# **Run this code for example output:**\n",
        "\n",
        "```\n",
        "# Example usage:\n",
        "img_path = \"/content/drive/MyDrive/EyeDataset/Testing Images/1000_left.jpg\"\n",
        "label_path = \"/content/drive/MyDrive/EyeDataset/Testing Images/1000_left.jpg\"\n",
        "\n",
        "img = imread(img_path)\n",
        "label = imread(label_path)\n",
        "\n",
        "radius_list = []\n",
        "centre_list_w = []\n",
        "centre_list_h = []\n",
        "\n",
        "result_img, borders, mask, processed_label, radius_list, centre_list_w, centre_list_h = process_without_gb(img, label, radius_list, centre_list_w, centre_list_h)\n",
        "\n",
        "# Display the input image\n",
        "cv2_imshow(img)\n",
        "cv2.waitKey(0)\n",
        "cv2.destroyAllWindows()\n",
        "\n",
        "# Display the output image\n",
        "cv2_imshow(result_img)\n",
        "cv2.waitKey(0)\n",
        "cv2.destroyAllWindows()\n",
        "\n",
        "print(\"Borders:\")\n",
        "print(borders)\n",
        "print(\"Mask:\")\n",
        "print(mask)\n",
        "print(\"Processed Label:\")\n",
        "print(processed_label)\n",
        "print(\"Radii List:\")\n",
        "print(radius_list)\n",
        "print(\"Center List W:\")\n",
        "print(centre_list_w)\n",
        "print(\"Center List H:\")\n",
        "print(centre_list_h)\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4EU8sIlVJNC"
      },
      "source": [
        "# **Save preprocessed Image and Metadata**\n",
        "\n",
        "The script then iterates through each image in the training folder, processes the image using a function named process_without_gb, and extracts relevant information such as radius and center coordinates. The script saves the preprocessed image, appends details to lists, and prints debugging information. For each image, a new DataFrame is created, concatenated with the existing data, and the updated dataset is written to the CSV file. This script streamlines the preprocessing and organization of eye-related image data for subsequent analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sa1EuZcHvqxC"
      },
      "outputs": [],
      "source": [
        "# Set your training and preprocessed image folders\n",
        "training_folder = \"/content/drive/MyDrive/EyeDataset/Training Images/\"\n",
        "preprocessed_folder = \"/content/drive/MyDrive/EyeDataset/Preprocessed Images/\"\n",
        "csv_info_file = \"/content/drive/MyDrive/EyeDataset/stage2data.csv\"\n",
        "\n",
        "# Ensure the preprocessed folder exists\n",
        "fold_dir(preprocessed_folder)\n",
        "\n",
        "csv_columns = [\"Name\", \"Radius\", \"Center_W\", \"Center_H\", \"Scale\", \"Scale Resolution\"]\n",
        "\n",
        "# Check if the CSV file already exists\n",
        "if os.path.exists(csv_info_file):\n",
        "    # Load the existing data from the CSV file\n",
        "    data_df = pd.read_csv(csv_info_file)\n",
        "else:\n",
        "    # Initialize an empty DataFrame if the CSV file doesn't exist\n",
        "    data_df = pd.DataFrame(columns=csv_columns)\n",
        "\n",
        "# Iterate through each image in the training folder\n",
        "for filename in os.listdir(training_folder):\n",
        "    if filename.endswith(\".jpg\") or filename.endswith(\".png\"):  # Update the file extensions if needed\n",
        "        img_path = os.path.join(training_folder, filename)\n",
        "        label_path = img_path  # Update this based on your label image paths\n",
        "\n",
        "        img = imread(img_path)\n",
        "        label = imread(label_path)\n",
        "\n",
        "        # Initialize lists for each image\n",
        "        radius_list = []\n",
        "        centre_list_w = []\n",
        "        centre_list_h = []\n",
        "        name_list = []\n",
        "\n",
        "        # Process the image\n",
        "        result_img, borders, mask, processed_label, radius_list, centre_list_w, centre_list_h = process_without_gb(img, label, radius_list, centre_list_w, centre_list_h)\n",
        "\n",
        "        # Additional processing for scale_list\n",
        "        scale_list = [a * 2 / 912 for a in radius_list]\n",
        "\n",
        "        # Save preprocessed image\n",
        "        preprocessed_img_path = os.path.join(preprocessed_folder, filename)\n",
        "        result_img = result_img.astype(np.uint8)\n",
        "        imwrite(preprocessed_img_path, result_img)\n",
        "        # Print a success message\n",
        "        print(\"Image successfully written to:\", preprocessed_img_path)\n",
        "\n",
        "        # Modify the image path and append to name_list\n",
        "        name_list.append(filename)\n",
        "\n",
        "\n",
        "        # Print values for debugging\n",
        "        print(\"name_list[0]:\", name_list[0])\n",
        "        print(\"radius_list[0]:\", radius_list[0])\n",
        "        print(\"centre_list_w[0]:\", centre_list_w[0])\n",
        "        print(\"centre_list_h[0]:\", centre_list_h[0])\n",
        "        print(\"scale_list[0]:\", scale_list[0])\n",
        "        print(\"scale_resolution[0]:\", scale_list[0] * 1000)\n",
        "\n",
        "        # Create a new DataFrame with the current image data\n",
        "        new_data_df = pd.DataFrame({\n",
        "            \"Name\": [name_list[0]],\n",
        "            \"Radius\": [radius_list[0]],\n",
        "            \"Center_W\": [centre_list_w[0]],\n",
        "            \"Center_H\": [centre_list_h[0]],\n",
        "            \"Scale\":[scale_list[0]],\n",
        "            \"Scale Resolution\": [scale_list[0] * 1000]\n",
        "        })\n",
        "\n",
        "        # Concatenate the new DataFrame with the existing data_df\n",
        "        data_df = pd.concat([data_df, new_data_df], ignore_index=True)\n",
        "\n",
        "        # Write information to CSV file after each iteration\n",
        "        data_df.to_csv(csv_info_file, index=False, encoding='utf-8')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tiMIrSagNuWI"
      },
      "source": [
        "# **Image Quality Grading**\n",
        "\n",
        "The code loads a pre-trained ResNet model ([ EyeQ_Assesment ](https://colab.research.google.com/drive/1NQfqSr70qeoJQaT-wSaDZ3sEAp3RX4NW?usp=sharing)) for binary classification of eye images as good or bad quality. It processes image names from a CSV file, makes predictions, adds a 'quality' column to the DataFrame, and saves the updated data to the same CSV file. Error handling accounts for missing images or processing issues."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define a transform to preprocess the images\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Adjust size based on your model's input size\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Load the saved model\n",
        "loaded_model = torch.load('/content/drive/MyDrive/EyeDataset/EyeQ_resnet_model.pth')\n",
        "loaded_model.eval()\n",
        "\n",
        "# Load and preprocess the test image\n",
        "test_image_path = '/content/images (1).jpeg'  # Replace with the actual path to your test image\n",
        "test_image = Image.open(test_image_path).convert('RGB')\n",
        "input_tensor = transform(test_image)\n",
        "input_batch = input_tensor.unsqueeze(0)  # Add batch dimension as model expects batches\n",
        "\n",
        "# If using GPU, move the input tensor to the GPU\n",
        "if torch.cuda.is_available():\n",
        "    input_batch = input_batch.to('cuda')\n",
        "\n",
        "# Perform inference\n",
        "with torch.no_grad():\n",
        "    output = loaded_model(input_batch)\n",
        "    predictions = torch.sigmoid(output) > 0.5  # Use sigmoid with threshold 0.5 for binary predictions\n",
        "\n",
        "# Map the predicted class index to the actual class label\n",
        "class_labels = {0: 'Good', 1: 'Bad'}  # Update with your actual class labels\n",
        "\n",
        "# Convert predictions to class labels\n",
        "predicted_class = class_labels[predictions.item()]\n",
        "\n",
        "# Display the test image and predicted class\n",
        "plt.imshow(test_image)\n",
        "plt.title(f'Predicted Class: {predicted_class}')\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "kAm6JShEjv9E",
        "outputId": "46b588e8-324d-448d-b5b6-3b0865607e88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CgeExS4GeJ1o"
      },
      "outputs": [],
      "source": [
        "# Define a transform to preprocess the images\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Adjust size based on your model's input size\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Load the saved model\n",
        "loaded_model = torch.load('/content/drive/MyDrive/EyeDataset/EyeQ_resnet_model.pth')\n",
        "loaded_model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "THuNA73oci4y"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchsummary import summary\n",
        "\n",
        "# Assuming loaded_model is your PyTorch model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "loaded_model = loaded_model.to(device)\n",
        "\n",
        "# Print the model summary\n",
        "summary(loaded_model, input_size=(3, 224, 224))  # Adjust input_size accordingly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XvNlGgGJDJfL"
      },
      "outputs": [],
      "source": [
        "# Load the CSV file with image names\n",
        "csv_path = '/content/drive/MyDrive/EyeDataset/stage2data.csv'\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# Create a new column for predictions\n",
        "df['quality'] = 0\n",
        "\n",
        "# Loop through the rows and make predictions\n",
        "for index, row in df.iterrows():\n",
        "    image_path = '/content/drive/MyDrive/EyeDataset/Preprocessed Images/' + row['Name']\n",
        "\n",
        "    try:\n",
        "        # Check if the image file exists\n",
        "        if not os.path.exists(image_path):\n",
        "            print(f\"Image not found: {image_path}\")\n",
        "            continue\n",
        "\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "        image = transform(image)\n",
        "        image = image.unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "        # Make predictions\n",
        "        with torch.no_grad():\n",
        "            output = loaded_model(image)\n",
        "            prediction = torch.sigmoid(output) > 0.5\n",
        "            df.at[index, 'quality'] = int(prediction.item())\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing image {image_path}: {e}\")\n",
        "        continue\n",
        "\n",
        "# Save the updated DataFrame with predictions\n",
        "df.to_csv('/content/drive/MyDrive/EyeDataset/stage2data.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPXtPmbUNziq"
      },
      "source": [
        "# **Enhancing Bad Quality Images**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPMvn2pZBcrm"
      },
      "source": [
        "# **Defining Necessary Functions for RetFound**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DDcRyFQWOAj6"
      },
      "outputs": [],
      "source": [
        "class VisionTransformer(timm.models.vision_transformer.VisionTransformer):\n",
        "    \"\"\" Vision Transformer with support for global average pooling\n",
        "    \"\"\"\n",
        "    def __init__(self, global_pool=False, **kwargs):\n",
        "        super(VisionTransformer, self).__init__(**kwargs)\n",
        "\n",
        "        self.global_pool = global_pool\n",
        "        if self.global_pool:\n",
        "            norm_layer = kwargs['norm_layer']\n",
        "            embed_dim = kwargs['embed_dim']\n",
        "            self.fc_norm = norm_layer(embed_dim)\n",
        "\n",
        "            del self.norm  # remove the original norm\n",
        "\n",
        "    def forward_features(self, x):\n",
        "        B = x.shape[0]\n",
        "        x = self.patch_embed(x)\n",
        "\n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)  # stole cls_tokens impl from Phil Wang, thanks\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "        x = x + self.pos_embed\n",
        "        x = self.pos_drop(x)\n",
        "\n",
        "        for blk in self.blocks:\n",
        "            x = blk(x)\n",
        "\n",
        "        if self.global_pool:\n",
        "            x = x[:, 1:, :].mean(dim=1)  # global pool without cls token\n",
        "            outcome = self.fc_norm(x)\n",
        "        else:\n",
        "            x = self.norm(x)\n",
        "            outcome = x[:, 0]\n",
        "\n",
        "        return outcome\n",
        "\n",
        "\n",
        "def vit_large_patch16(**kwargs):\n",
        "    model = VisionTransformer(\n",
        "        patch_size=16, embed_dim=1024, depth=24, num_heads=16, mlp_ratio=4, qkv_bias=True,\n",
        "        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
        "    return model\n",
        "\n",
        "def get_2d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):\n",
        "    \"\"\"\n",
        "    grid_size: int of the grid height and width\n",
        "    return:\n",
        "    pos_embed: [grid_size*grid_size, embed_dim] or [1+grid_size*grid_size, embed_dim] (w/ or w/o cls_token)\n",
        "    \"\"\"\n",
        "    grid_h = np.arange(grid_size, dtype=np.float32)\n",
        "    grid_w = np.arange(grid_size, dtype=np.float32)\n",
        "    grid = np.meshgrid(grid_w, grid_h)  # here w goes first\n",
        "    grid = np.stack(grid, axis=0)\n",
        "\n",
        "    grid = grid.reshape([2, 1, grid_size, grid_size])\n",
        "    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
        "    if cls_token:\n",
        "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
        "    return pos_embed\n",
        "\n",
        "\n",
        "def get_2d_sincos_pos_embed_from_grid(embed_dim, grid):\n",
        "    assert embed_dim % 2 == 0\n",
        "\n",
        "    # use half of dimensions to encode grid_h\n",
        "    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])  # (H*W, D/2)\n",
        "    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])  # (H*W, D/2)\n",
        "\n",
        "    emb = np.concatenate([emb_h, emb_w], axis=1) # (H*W, D)\n",
        "    return emb\n",
        "\n",
        "\n",
        "def get_1d_sincos_pos_embed_from_grid(embed_dim, pos):\n",
        "    \"\"\"\n",
        "    embed_dim: output dimension for each position\n",
        "    pos: a list of positions to be encoded: size (M,)\n",
        "    out: (M, D)\n",
        "    \"\"\"\n",
        "    assert embed_dim % 2 == 0\n",
        "    omega = np.arange(embed_dim // 2, dtype=np.float)\n",
        "    omega /= embed_dim / 2.\n",
        "    omega = 1. / 10000**omega  # (D/2,)\n",
        "\n",
        "    pos = pos.reshape(-1)  # (M,)\n",
        "    out = np.einsum('m,d->md', pos, omega)  # (M, D/2), outer product\n",
        "\n",
        "    emb_sin = np.sin(out) # (M, D/2)\n",
        "    emb_cos = np.cos(out) # (M, D/2)\n",
        "\n",
        "    emb = np.concatenate([emb_sin, emb_cos], axis=1)  # (M, D)\n",
        "    return emb\n",
        "\n",
        "def interpolate_pos_embed(model, checkpoint_model):\n",
        "    if 'pos_embed' in checkpoint_model:\n",
        "        pos_embed_checkpoint = checkpoint_model['pos_embed']\n",
        "        embedding_size = pos_embed_checkpoint.shape[-1]\n",
        "        num_patches = model.patch_embed.num_patches\n",
        "        num_extra_tokens = model.pos_embed.shape[-2] - num_patches\n",
        "        # height (== width) for the checkpoint position embedding\n",
        "        orig_size = int((pos_embed_checkpoint.shape[-2] - num_extra_tokens) ** 0.5)\n",
        "        # height (== width) for the new position embedding\n",
        "        new_size = int(num_patches ** 0.5)\n",
        "        # class_token and dist_token are kept unchanged\n",
        "        if orig_size != new_size:\n",
        "            print(\"Position interpolate from %dx%d to %dx%d\" % (orig_size, orig_size, new_size, new_size))\n",
        "            extra_tokens = pos_embed_checkpoint[:, :num_extra_tokens]\n",
        "            # only the position tokens are interpolated\n",
        "            pos_tokens = pos_embed_checkpoint[:, num_extra_tokens:]\n",
        "            pos_tokens = pos_tokens.reshape(-1, orig_size, orig_size, embedding_size).permute(0, 3, 1, 2)\n",
        "            pos_tokens = torch.nn.functional.interpolate(\n",
        "                pos_tokens, size=(new_size, new_size), mode='bicubic', align_corners=False)\n",
        "            pos_tokens = pos_tokens.permute(0, 2, 3, 1).flatten(1, 2)\n",
        "            new_pos_embed = torch.cat((extra_tokens, pos_tokens), dim=1)\n",
        "            checkpoint_model['pos_embed'] = new_pos_embed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_C_bMPML9vJ"
      },
      "source": [
        "# **Defining Teacher and Student Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UEpy3j_WStDz"
      },
      "outputs": [],
      "source": [
        "def create_teacher_model():\n",
        "    # Instantiate the ViT model\n",
        "    model = vit_large_patch16(\n",
        "        num_classes=8,\n",
        "        drop_path_rate=0.2,\n",
        "        global_pool=True,\n",
        "    )\n",
        "    checkpoint = torch.load('/content/drive/MyDrive/EyeDataset/RETFound_cfp_weights.pth', map_location='cpu')\n",
        "    checkpoint_model = checkpoint['model']\n",
        "    state_dict = model.state_dict()\n",
        "\n",
        "    for k in ['head.weight', 'head.bias']:\n",
        "        if k in checkpoint_model and checkpoint_model[k].shape != state_dict[k].shape:\n",
        "            print(f\"Removing key {k} from pretrained checkpoint\")\n",
        "            del checkpoint_model[k]\n",
        "\n",
        "    interpolate_pos_embed(model, checkpoint_model)\n",
        "    msg = model.load_state_dict(checkpoint_model, strict=False)\n",
        "    assert set(msg.missing_keys) == {'head.weight', 'head.bias', 'fc_norm.weight', 'fc_norm.bias'}\n",
        "    return model\n",
        "\n",
        "# Define the ResNet model as the student\n",
        "class ResNetStudent(nn.Module):\n",
        "    def __init__(self, num_classes=8):\n",
        "        super(ResNetStudent, self).__init__()\n",
        "        self.resnet = models.resnet18(pretrained=True)\n",
        "        num_ftrs = self.resnet.fc.in_features\n",
        "        self.resnet.fc = nn.Linear(num_ftrs, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.resnet(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6amxRlSrJUe"
      },
      "source": [
        "# **Setting Up The Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LssxrCNVrRMQ"
      },
      "outputs": [],
      "source": [
        "class EyeDiseaseDataset(Dataset):\n",
        "    def __init__(self, csv_file, root_dir, transform=None):\n",
        "        self.data = pd.read_csv(csv_file)\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = os.path.join(self.root_dir, self.data.iloc[idx, 4])  # Right-Fundus column\n",
        "        try:\n",
        "            image = Image.open(img_name).convert('RGB')\n",
        "        except FileNotFoundError:\n",
        "            # Handle case where image is not found\n",
        "            # print(f\"Image '{img_name}' not found. Skipping...\")\n",
        "            return self.__getitem__((idx + 1) % len(self))  # Call __getitem__ recursively with the next index\n",
        "\n",
        "        keywords = self.data.iloc[idx, 6]  # Assuming the right diagnostic keywords are in column 6\n",
        "        labels = torch.tensor(eval(self.data.iloc[idx, -1]), dtype=torch.float32)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, keywords, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vNKedwAcCfM1"
      },
      "outputs": [],
      "source": [
        "# Define data augmentation and transformations\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomRotation(30),  # Rotate by up to 30 degrees\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),  # Randomly resize and crop\n",
        "        transforms.RandomApply([transforms.RandomAffine(0, translate=(0.1, 0.1))], p=0.5),  # Randomly apply affine transformation\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val_test': transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Olk04wouCjoe"
      },
      "outputs": [],
      "source": [
        "def target_to_one_hot(targets):\n",
        "    target_classes = ['N', 'D', 'G', 'C', 'A', 'H', 'M', 'O']\n",
        "    one_hot_targets = []\n",
        "    for target in targets:\n",
        "        target_index = torch.argmax(target).item()\n",
        "        one_hot_target = [0] * len(target_classes)\n",
        "        one_hot_target[target_index] = 1\n",
        "        one_hot_targets.append(one_hot_target)\n",
        "    return torch.tensor(one_hot_targets)\n",
        "\n",
        "def one_hot_to_label(one_hot_targets):\n",
        "    labels = []\n",
        "    for one_hot_target in one_hot_targets:\n",
        "        label = np.argmax(one_hot_target)\n",
        "        labels.append(label)\n",
        "    return labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "dXcxlvxdCoR7",
        "outputId": "0c5c8caf-9cf0-44df-bc99-994f48539175"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Image '/content/drive/MyDrive/EyeDataset/Preprocessed Images/2174_right.jpg' not found. Skipping...\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "object of type 'NoneType' has no len()",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-0eae8fad8c3f>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meye_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meye_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mclass_counts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load dataset\n",
        "csv_file = \"/content/drive/MyDrive/EyeDataset/full.csv\"\n",
        "root_dir = \"/content/drive/MyDrive/EyeDataset/Preprocessed Images\"\n",
        "eye_dataset = EyeDiseaseDataset(csv_file=csv_file, root_dir=root_dir, transform=data_transforms['train'])\n",
        "\n",
        "# Calculate class proportions dynamically\n",
        "class_counts = {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0}\n",
        "for idx in range(len(eye_dataset)):\n",
        "    _, _, labels = eye_dataset[idx]\n",
        "    for i in range(len(labels)):\n",
        "        class_counts[i] += labels[i].item()\n",
        "\n",
        "total_samples = len(eye_dataset)\n",
        "class_proportions = {key: count / total_samples for key, count in class_counts.items()}\n",
        "print(\"Class Proportions:\", class_proportions)\n",
        "\n",
        "# Split dataset into training, validation, and test sets\n",
        "train_size = 0.8\n",
        "val_size = 0.1\n",
        "test_size = 0.1\n",
        "\n",
        "train_indices, val_test_indices = train_test_split(list(range(len(eye_dataset))), train_size=train_size, random_state=42)\n",
        "val_indices, test_indices = train_test_split(val_test_indices, train_size=val_size / (val_size + test_size), random_state=42)\n",
        "\n",
        "# Define data loaders for training, validation, and test sets\n",
        "def create_data_loader(dataset, indices, batch_size):\n",
        "    sampler = SubsetRandomSampler(indices)\n",
        "    return DataLoader(dataset, batch_size=batch_size, sampler=sampler)\n",
        "\n",
        "batch_size = 32\n",
        "train_loader = create_data_loader(eye_dataset, train_indices, batch_size)\n",
        "val_loader = create_data_loader(eye_dataset, val_indices, batch_size)\n",
        "test_loader = create_data_loader(eye_dataset, test_indices, batch_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 602
        },
        "id": "gBmdhn6gyqkd",
        "outputId": "fe874c95-3298-46fa-c07b-957638f79c35"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
            "Epoch 1/20: 100%|| 160/160 [28:46<00:00, 10.79s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold [1/5], Epoch [1/20], Validation Loss: 0.2779, Validation Accuracy: 0.4504\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/20: 100%|| 160/160 [28:04<00:00, 10.53s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold [1/5], Epoch [2/20], Validation Loss: 0.2768, Validation Accuracy: 0.4504\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/20: 100%|| 160/160 [27:30<00:00, 10.31s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold [1/5], Epoch [3/20], Validation Loss: 0.2511, Validation Accuracy: 0.4504\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/20: 100%|| 160/160 [27:56<00:00, 10.48s/it]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-60cf2037967e>\u001b[0m in \u001b[0;36m<cell line: 32>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeywords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m                 \u001b[0mval_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36m_forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavgpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0midentity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    454\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 456\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    457\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
        "from torchvision import models\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# Load dataset\n",
        "csv_file = \"/content/drive/MyDrive/EyeDataset/full.csv\"\n",
        "root_dir = \"/content/drive/MyDrive/EyeDataset/Preprocessed Images\"\n",
        "eye_dataset = EyeDiseaseDataset(csv_file=csv_file, root_dir=root_dir, transform=data_transforms['train'])\n",
        "\n",
        "# Define ResNet model\n",
        "model = models.resnet18(pretrained=True)\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, 8)  # 8 classes\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Define hyperparameters\n",
        "num_epochs = 20\n",
        "batch_size = 32\n",
        "\n",
        "# Initialize cross-validation\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Iterate over folds for cross-validation\n",
        "for fold, (train_idx, val_idx) in enumerate(skf.split(range(len(eye_dataset)), eye_dataset.data['target'])):\n",
        "    train_sampler = SubsetRandomSampler(train_idx)\n",
        "    val_sampler = SubsetRandomSampler(val_idx)\n",
        "\n",
        "    train_loader = DataLoader(eye_dataset, batch_size=batch_size, sampler=train_sampler)\n",
        "    val_loader = DataLoader(eye_dataset, batch_size=batch_size, sampler=val_sampler)\n",
        "\n",
        "    # Define scheduler for learning rate adjustment\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n",
        "\n",
        "    # Train the model\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        for images, keywords, targets in tqdm(train_loader, desc=f'Epoch {epoch + 1}/{num_epochs}'):\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Validate the model\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_preds = []\n",
        "        val_targets = []\n",
        "        with torch.no_grad():\n",
        "            for images, keywords, targets in val_loader:\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, targets)\n",
        "                val_loss += loss.item() * images.size(0)\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                val_preds.extend(predicted.cpu().numpy())\n",
        "                val_targets.extend(targets.cpu().numpy())\n",
        "\n",
        "        val_loss /= len(val_loader.dataset)\n",
        "        # Convert one-hot encoded targets back to labels\n",
        "        val_labels = one_hot_to_label(val_targets)\n",
        "        val_preds_labels = one_hot_to_label(val_preds)\n",
        "\n",
        "        # Calculate accuracy using label-encoded targets and predictions\n",
        "        val_accuracy = accuracy_score(val_labels, val_preds_labels)\n",
        "\n",
        "        # Update learning rate\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        print(f'Fold [{fold+1}/{skf.n_splits}], Epoch [{epoch+1}/{num_epochs}], '\n",
        "              f'Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n",
        "\n",
        "# Evaluate on test set\n",
        "test_loss = 0.0\n",
        "test_preds = []\n",
        "test_targets = []\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for images, keywords, targets in test_loader:\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, targets)\n",
        "        test_loss += loss.item() * images.size(0)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        test_preds.extend(predicted.cpu().numpy())\n",
        "        test_targets.extend(targets.cpu().numpy())\n",
        "\n",
        "test_loss /= len(test_loader.dataset)\n",
        "test_accuracy = accuracy_score(test_targets, test_preds)\n",
        "test_precision = precision_score(test_targets, test_preds, average='macro')\n",
        "test_recall = recall_score(test_targets, test_preds, average='macro')\n",
        "test_f1 = f1_score(test_targets, test_preds, average='macro')\n",
        "test_conf_matrix = confusion_matrix(test_targets, test_preds)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.4f}, '\n",
        "      f'Test Accuracy: {test_accuracy:.4f}, '\n",
        "      f'Test Precision: {test_precision:.4f}, '\n",
        "      f'Test Recall: {test_recall:.4f}, '\n",
        "      f'Test F1-score: {test_f1:.4f}')\n",
        "\n",
        "# Define the class labels based on your target classes\n",
        "classes = ['N', 'D', 'G', 'C', 'A', 'H', 'M', 'O']\n",
        "\n",
        "# Plotting the confusion matrix for the test set as a heatmap\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(test_conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
        "plt.xlabel('Predicted labels')\n",
        "plt.ylabel('True labels')\n",
        "plt.title('Test Confusion Matrix')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "631Ffp_ODEYt",
        "outputId": "b6890f12-22aa-4834-db25-719b7891c3da"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ],
      "source": [
        "# Instantiate the teacher model\n",
        "teacher_model = create_teacher_model()\n",
        "\n",
        "# Instantiate the ResNet student model\n",
        "student_model = ResNetStudent(num_classes=8)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(student_model.parameters(), lr=0.001)\n",
        "\n",
        "# Train the student model\n",
        "num_epochs = 30\n",
        "temperature = 3  # Temperature parameter for distillation\n",
        "\n",
        "# Define distillation loss function\n",
        "def distillation_loss(outputs_student, outputs_teacher, temperature=3):\n",
        "    soft_targets = nn.functional.softmax(outputs_teacher / temperature, dim=1)\n",
        "    return nn.KLDivLoss()(nn.functional.log_softmax(outputs_student / temperature, dim=1), soft_targets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XO4PRTbJEf6q"
      },
      "source": [
        "# **Training and Testing Loop**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "DrBgXLC7KSaZ",
        "outputId": "841f9319-943f-438c-f173-139728111147"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input tensor shape: torch.Size([1, 3, 224, 224])\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "Given normalized_shape=[1024], expected input with shape [*, 1024], but got input of size[1]",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-b57f5175d54b>\u001b[0m in \u001b[0;36m<cell line: 24>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# No need to compute gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mteacher_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Set teacher model to evaluation mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0moutputs_teacher\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mteacher_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# Now you can print or analyze the outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/timm/models/vision_transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    703\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/timm/models/vision_transformer.py\u001b[0m in \u001b[0;36mforward_head\u001b[0;34m(self, x, pre_logits)\u001b[0m\n\u001b[1;32m    697\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_pool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# class token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_drop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpre_logits\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/normalization.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m         return F.layer_norm(\n\u001b[0m\u001b[1;32m    202\u001b[0m             input, self.normalized_shape, self.weight, self.bias, self.eps)\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlayer_norm\u001b[0;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[1;32m   2544\u001b[0m             \u001b[0mlayer_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalized_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2545\u001b[0m         )\n\u001b[0;32m-> 2546\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalized_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2547\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Given normalized_shape=[1024], expected input with shape [*, 1024], but got input of size[1]"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "\n",
        "# Load the image\n",
        "image_path = \"/content/drive/MyDrive/EyeDataset/Preprocessed Images/1005_left.jpg\"  # Replace with the path to your image file\n",
        "image = Image.open(image_path)\n",
        "\n",
        "# Define preprocessing transforms\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Resize the image to match the input size expected by the teacher model\n",
        "    transforms.ToTensor(),  # Convert the image to a PyTorch tensor\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize the image\n",
        "])\n",
        "\n",
        "# Apply the preprocessing transforms to the image\n",
        "input_tensor = preprocess(image)\n",
        "input_tensor = input_tensor.unsqueeze(0)  # Add a batch dimension as the teacher model expects a batch of images\n",
        "\n",
        "# Print the shape of the input tensor for debugging\n",
        "print(\"Input tensor shape:\", input_tensor.shape)\n",
        "\n",
        "# Pass the input tensor through the teacher model\n",
        "with torch.no_grad():  # No need to compute gradients\n",
        "    teacher_model.eval()  # Set teacher model to evaluation mode\n",
        "    outputs_teacher = teacher_model(input_tensor)\n",
        "\n",
        "# Now you can print or analyze the outputs\n",
        "print(outputs_teacher)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "82VcNym75IQY"
      },
      "outputs": [],
      "source": [
        "for epoch in range(num_epochs):\n",
        "    print(f'Starting epoch {epoch + 1}/{num_epochs}')\n",
        "    student_model.train()\n",
        "    teacher_model.eval()\n",
        "    running_loss = 0.0\n",
        "    for i, (images, _, targets) in enumerate(tqdm(train_loader)):\n",
        "        optimizer.zero_grad()\n",
        "        outputs_student = student_model(images)\n",
        "        with torch.no_grad():  # No need for gradients for teacher model\n",
        "                outputs_teacher = teacher_model(images)\n",
        "\n",
        "\n",
        "        # Calculate classification loss\n",
        "        loss_cls = criterion(outputs_student, targets)\n",
        "\n",
        "        # Calculate distillation loss\n",
        "        loss_distil = distillation_loss(outputs_student, outputs_teacher, temperature)\n",
        "\n",
        "        # Combine both losses\n",
        "        loss = loss_cls + loss_distil\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader.dataset)\n",
        "\n",
        "    student_model.eval()\n",
        "    val_loss = 0.0\n",
        "    val_preds = []\n",
        "    val_targets = []\n",
        "    with torch.no_grad():\n",
        "        for images, _, targets in val_loader:\n",
        "            outputs = student_model(images)\n",
        "            loss = criterion(outputs, targets)\n",
        "            val_loss += loss.item() * images.size(0)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            val_preds.extend(predicted.cpu().numpy())\n",
        "            val_targets.extend(targets.cpu().numpy())\n",
        "    val_loss /= len(val_loader.dataset)\n",
        "    val_accuracy = accuracy_score(val_targets, val_preds)\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
        "          f'Training Loss: {epoch_loss:.4f}, '\n",
        "          f'Validation Loss: {val_loss:.4f}, '\n",
        "          f'Validation Accuracy: {val_accuracy:.4f}')\n",
        "\n",
        "# Evaluate the student model on the test set\n",
        "test_loss = 0.0\n",
        "test_preds = []\n",
        "test_targets = []\n",
        "student_model.eval()\n",
        "with torch.no_grad():\n",
        "    for images, _, targets in tqdm(test_loader):\n",
        "        outputs = student_model(images)\n",
        "        loss = criterion(outputs, targets)\n",
        "        test_loss += loss.item() * images.size(0)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        test_preds.extend(predicted.cpu().numpy())\n",
        "        test_targets.extend(targets.cpu().numpy())\n",
        "\n",
        "test_loss /= len(test_loader.dataset)\n",
        "test_accuracy = accuracy_score(test_targets, test_preds)\n",
        "test_precision = precision_score(test_targets, test_preds, average='macro')\n",
        "test_recall = recall_score(test_targets, test_preds, average='macro')\n",
        "test_f1 = f1_score(test_targets, test_preds, average='macro')\n",
        "test_conf_matrix = confusion_matrix(test_targets, test_preds)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.4f}, '\n",
        "      f'Test Accuracy: {test_accuracy:.4f}, '\n",
        "      f'Test Precision: {test_precision:.4f}, '\n",
        "      f'Test Recall: {test_recall:.4f}, '\n",
        "      f'Test F1-score: {test_f1:.4f}')\n",
        "\n",
        "# Define the class labels based on your target classes\n",
        "classes = ['N', 'D', 'G', 'C', 'A', 'H', 'M', 'O']\n",
        "\n",
        "# Plotting the confusion matrix for the test set as a heatmap\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(test_conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
        "plt.xlabel('Predicted labels')\n",
        "plt.ylabel('True labels')\n",
        "plt.title('Test Confusion Matrix')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "FtylypXkDFTj",
        "outputId": "a8d0f77e-f11d-4143-8757-869e971d3335"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-d37b5ca9ce52>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSubsetRandomSampler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStratifiedKFold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_meta_registrations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mextension\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_HAS_OPS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/models/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0malexnet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mconvnext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdensenet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mefficientnet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mgooglenet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/models/convnext.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmisc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConv2dNormActivation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPermute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstochastic_depth\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStochasticDepth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_presets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImageClassification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/ops/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_register_onnx_ops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_register_custom_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m from .boxes import (\n\u001b[1;32m      3\u001b[0m     \u001b[0mbatched_nms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mbox_area\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mbox_convert\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/ops/_register_onnx_ops.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monnx\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msymbolic_opset11\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopset11\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msymbolic_helper\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mparse_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/onnx/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     44\u001b[0m )\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m from ._internal.exporter import (  # usort:skip. needs to be last to avoid circular import\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0mDiagnosticOptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mExportOptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/onnx/_internal/exporter.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_internal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_beartype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mio_adapter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_internal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiagnostics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minfra\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m from torch.onnx._internal.fx import (\n\u001b[0m\u001b[1;32m     45\u001b[0m     \u001b[0mdecomposition_table\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mpatcher\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpatcher\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/onnx/_internal/fx/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpatcher\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mONNXTorchPatcher\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mserialization\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msave_model_with_external_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m __all__ = [\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/onnx/_internal/fx/patcher.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# safetensors is not an exporter requirement, but needed for some huggingface models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0msafetensors\u001b[0m  \u001b[0;31m# type: ignore[import]  # noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mtransformers\u001b[0m  \u001b[0;31m# type: ignore[import]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0msafetensors\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msafetensors_torch\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# Check the dependencies satisfy the minimal versions required.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdependency_versions_check\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m from .utils import (\n\u001b[1;32m     28\u001b[0m     \u001b[0mOptionalDependencyNotAvailable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/dependency_versions_check.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdependency_versions_table\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrequire_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequire_version_core\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# limitations under the License.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mhuggingface_hub\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_full_repo_name\u001b[0m  \u001b[0;31m# for backward compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhuggingface_hub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstants\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHF_HUB_DISABLE_TELEMETRY\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mDISABLE_TELEMETRY\u001b[0m  \u001b[0;31m# for backward compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpackaging\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mattr_to_modules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m             \u001b[0msubmod_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{package_name}.{attr_to_modules[name]}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m             \u001b[0msubmod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubmod_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    371\u001b[0m             \u001b[0mattr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubmod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/hf_api.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     61\u001b[0m )\n\u001b[1;32m     62\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_inference_endpoints\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInferenceEndpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mInferenceEndpointType\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m from ._multi_commits import (\n\u001b[0m\u001b[1;32m     64\u001b[0m     \u001b[0mMULTI_COMMIT_PR_CLOSE_COMMENT_FAILURE_BAD_REQUEST_TEMPLATE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0mMULTI_COMMIT_PR_CLOSE_COMMENT_FAILURE_NO_CHANGES_TEMPLATE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/_multi_commits.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mdataclass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mMultiCommitStrategy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m     \"\"\"Dataclass containing a list of [`MultiCommitStep`] to commit iteratively.\n\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/dataclasses.py\u001b[0m in \u001b[0;36mdataclass\u001b[0;34m(cls, init, repr, eq, order, unsafe_hash, frozen, match_args, kw_only, slots)\u001b[0m\n\u001b[1;32m   1182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m     \u001b[0;31m# We're called as @dataclass without parens.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1184\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/dataclasses.py\u001b[0m in \u001b[0;36mwrap\u001b[0;34m(cls)\u001b[0m\n\u001b[1;32m   1173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1174\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1175\u001b[0;31m         return _process_class(cls, init, repr, eq, order, unsafe_hash,\n\u001b[0m\u001b[1;32m   1176\u001b[0m                               frozen, match_args, kw_only, slots)\n\u001b[1;32m   1177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/dataclasses.py\u001b[0m in \u001b[0;36m_process_class\u001b[0;34m(cls, init, repr, eq, order, unsafe_hash, frozen, match_args, kw_only, slots)\u001b[0m\n\u001b[1;32m   1051\u001b[0m         \u001b[0mother_tuple\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tuple_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'other'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m         _set_new_attribute(cls, '__eq__',\n\u001b[0;32m-> 1053\u001b[0;31m                            _cmp_fn('__eq__', '==',\n\u001b[0m\u001b[1;32m   1054\u001b[0m                                    \u001b[0mself_tuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother_tuple\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m                                    globals=globals))\n",
            "\u001b[0;32m/usr/lib/python3.10/dataclasses.py\u001b[0m in \u001b[0;36m_cmp_fn\u001b[0;34m(name, op, self_tuple, other_tuple, globals)\u001b[0m\n\u001b[1;32m    627\u001b[0m     \u001b[0;31m# '(other.x,other.y)'.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 629\u001b[0;31m     return _create_fn(name,\n\u001b[0m\u001b[1;32m    630\u001b[0m                       \u001b[0;34m(\u001b[0m\u001b[0;34m'self'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'other'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m                       [ 'if other.__class__ is self.__class__:',\n",
            "\u001b[0;32m/usr/lib/python3.10/dataclasses.py\u001b[0m in \u001b[0;36m_create_fn\u001b[0;34m(name, args, body, globals, locals, return_type)\u001b[0m\n\u001b[1;32m    430\u001b[0m     \u001b[0mtxt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"def __create_fn__({local_vars}):\\n{txt}\\n return {name}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[0mns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m     \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtxt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'__create_fn__'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mlocals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/_multi_commits.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
        "from torchvision import models\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Load dataset and calculate class proportions\n",
        "csv_file = \"/content/drive/MyDrive/EyeDataset/full.csv\"\n",
        "root_dir = \"/content/drive/MyDrive/EyeDataset/Preprocessed Images\"\n",
        "eye_dataset = EyeDiseaseDataset(csv_file=csv_file, root_dir=root_dir, transform=data_transforms['train'])\n",
        "\n",
        "# Calculate class proportions dynamically\n",
        "class_counts = {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0}\n",
        "for idx in range(len(eye_dataset)):\n",
        "    _, _, labels = eye_dataset[idx]\n",
        "    for i in range(len(labels)):\n",
        "        class_counts[i] += labels[i].item()\n",
        "\n",
        "total_samples = len(eye_dataset)\n",
        "class_proportions = {key: count / total_samples for key, count in class_counts.items()}\n",
        "print(\"Class Proportions:\", class_proportions)\n",
        "\n",
        "# Define ResNet model\n",
        "model = models.resnet18(pretrained=True)\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, 8)  # 8 classes\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Define hyperparameters\n",
        "num_epochs = 20\n",
        "batch_size = 32\n",
        "\n",
        "# Initialize cross-validation\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Iterate over folds for cross-validation\n",
        "for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
        "    train_sampler = SubsetRandomSampler(train_idx)\n",
        "    val_sampler = SubsetRandomSampler(val_idx)\n",
        "\n",
        "    train_loader = DataLoader(eye_dataset, batch_size=batch_size, sampler=train_sampler)\n",
        "    val_loader = DataLoader(eye_dataset, batch_size=batch_size, sampler=val_sampler)\n",
        "\n",
        "    # Define scheduler for learning rate adjustment\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n",
        "\n",
        "    # Train the model\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        for images, keywords, targets in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Validate the model\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_preds = []\n",
        "        val_targets = []\n",
        "        with torch.no_grad():\n",
        "            for images, keywords, targets in val_loader:\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, targets)\n",
        "                val_loss += loss.item() * images.size(0)\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                val_preds.extend(predicted.cpu().numpy())\n",
        "                val_targets.extend(targets.cpu().numpy())\n",
        "\n",
        "        val_loss /= len(val_loader.dataset)\n",
        "        val_accuracy = accuracy_score(val_targets, val_preds)\n",
        "\n",
        "        # Update learning rate\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        print(f'Fold [{fold+1}/{skf.n_splits}], Epoch [{epoch+1}/{num_epochs}], '\n",
        "              f'Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n",
        "\n",
        "    # Define test loader for each fold\n",
        "    test_loader = DataLoader(eye_dataset, batch_size=batch_size, sampler=val_sampler)\n",
        "\n",
        "    # Evaluate on test set\n",
        "    test_loss = 0.0\n",
        "    test_preds = []\n",
        "    test_targets = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for images, keywords, targets in test_loader:\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, targets)\n",
        "            test_loss += loss.item() * images.size(0)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            test_preds.extend(predicted.cpu().numpy())\n",
        "            test_targets.extend(targets.cpu().numpy())\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    test_accuracy = accuracy_score(test_targets, test_preds)\n",
        "    test_precision = precision_score(test_targets, test_preds, average='macro')\n",
        "    test_recall = recall_score(test_targets, test_preds, average='macro')\n",
        "    test_f1 = f1_score(test_targets, test_preds, average='macro')\n",
        "    test_conf_matrix = confusion_matrix(test_targets, test_preds)\n",
        "\n",
        "    print(f'Test Loss: {test_loss:.4f}, '\n",
        "          f'Test Accuracy: {test_accuracy:.4f}, '\n",
        "          f'Test Precision: {test_precision:.4f}, '\n",
        "          f'Test Recall: {test_recall:.4f}, '\n",
        "          f'Test F1-score: {test_f1:.4f}')\n",
        "\n",
        "    # Define the class labels based on your target classes\n",
        "    classes = ['N', 'D', 'G', 'C', 'A', 'H', 'M', 'O']\n",
        "\n",
        "    # Plotting the confusion matrix for the test set as a heatmap\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(test_conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
        "    plt.xlabel('Predicted labels')\n",
        "    plt.ylabel('True labels')\n",
        "    plt.title('Test Confusion Matrix')\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "voCHaP2alPla"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Subset, Dataset\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from torchvision.transforms import transforms, ColorJitter, ToPILImage\n",
        "import torchvision.models as models\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import torchvision.transforms.functional as F\n",
        "import os\n",
        "from torch.cuda.amp import autocast, GradScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_YUdM0C7vosA"
      },
      "outputs": [],
      "source": [
        "# Load your CSV file containing the dataset\n",
        "csv_file = '/content/drive/MyDrive/EyeDataset/ODR.csv'  # Update with your CSV file path\n",
        "data_df = pd.read_csv(csv_file)\n",
        "\n",
        "# Define the number of folds (k) for cross-validation\n",
        "num_folds = 4  # Adjust as needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TF4s5GPSn0Ot"
      },
      "outputs": [],
      "source": [
        "# Update the data transformation pipeline to include the random color spaces conversion\n",
        "data_transforms = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),  # Resize the image to 256x256\n",
        "    transforms.RandomChoice([\n",
        "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),  # Randomly adjust brightness, contrast, saturation, and hue\n",
        "        transforms.RandomGrayscale(p=0.2),  # Randomly convert the image to grayscale with a probability of 0.2\n",
        "    ]),\n",
        "    transforms.RandomApply([\n",
        "        transforms.GaussianBlur(kernel_size=3),  # Apply slight Gaussian blur with a kernel size of 3\n",
        "    ], p=0.2),\n",
        "    transforms.RandomRotation(degrees=(-10, 10)),  # Randomly rotate the image between -10 and 10 degrees\n",
        "    transforms.ToTensor(),  # Convert the image to a tensor\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize the image\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AAZBS3_Olday"
      },
      "outputs": [],
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data_df, root_dir, transform=None):\n",
        "        self.data_df = data_df\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.data_df.iloc[idx]['filename']  # Assuming 'Right-Fundus' is the column name for image filenames\n",
        "        img_path = os.path.join(self.root_dir, img_name)\n",
        "\n",
        "        try:\n",
        "            image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        except FileNotFoundError:\n",
        "            # Handle case where image is not found\n",
        "            # print(f\"Image '{img_name}' not found. Skipping...\")\n",
        "            return self.__getitem__((idx + 1) % len(self))  # Call __getitem__ recursively with the next index\n",
        "\n",
        "        # Apply transformations if specified\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        if 'right' in img_name.lower():\n",
        "            keywords = self.data_df.iloc[idx]['Right-Diagnostic Keywords'] if 'Right-Diagnostic Keywords' in self.data_df.columns else None\n",
        "        else:\n",
        "            keywords = self.data_df.iloc[idx]['Left-Diagnostic Keywords'] if 'Left-Diagnostic Keywords' in self.data_df.columns else None\n",
        "\n",
        "        # Convert single character target to numerical value\n",
        "        target_char = self.data_df.iloc[idx]['target']\n",
        "        target_dict = {'N': 0, 'D': 1, 'G': 2, 'C': 3, 'A': 4, 'H': 5, 'M': 6, 'O': 7}\n",
        "        target = torch.tensor(target_dict[target_char], dtype=torch.long)\n",
        "\n",
        "        return image, target, keywords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-XqEvYuqm5m5"
      },
      "outputs": [],
      "source": [
        "# Define your model architecture (e.g., MobileNet v2 with additional layers)\n",
        "class CustomMobileNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CustomMobileNet, self).__init__()\n",
        "        self.mobilenet = models.mobilenet_v2(pretrained=True)\n",
        "        self.fc = nn.Linear(1000, 8)  # Adjust num_classes based on your dataset\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.mobilenet(x)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XnVd29uAtKRK"
      },
      "outputs": [],
      "source": [
        "# Initialize StratifiedKFold with the specified number of folds\n",
        "stratified_kfold = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
        "\n",
        "# Define hyperparameters\n",
        "num_epochs = 10\n",
        "batch_size = 64\n",
        "learning_rate = 0.001\n",
        "dropout_rate = 0.3  # Adjust as needed"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = GradScaler()"
      ],
      "metadata": {
        "id": "tUQ1PMbueJhN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lMLgEfOekcRL",
        "outputId": "712f5d5d-4c92-407c-b299-2a25027e66a0"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1, Epoch 1, Training Loss: 1.7866757202148438\n",
            "Fold 1, Epoch 2, Training Loss: 1.5497190380096435\n",
            "Fold 1, Epoch 3, Training Loss: 1.4087752644220988\n",
            "Fold 1, Epoch 4, Training Loss: 1.3312245512008667\n",
            "Fold 1, Epoch 5, Training Loss: 1.2702170944213866\n",
            "Fold 1, Epoch 6, Training Loss: 1.233257253964742\n",
            "Fold 1, Epoch 7, Training Loss: 1.189457327524821\n",
            "Fold 1, Epoch 8, Training Loss: 1.1178558095296225\n",
            "Fold 1, Epoch 9, Training Loss: 1.085234960714976\n",
            "Fold 1, Epoch 10, Training Loss: 1.0220757031440735\n",
            "Fold 1, Validation Accuracy: 0.60450563204005\n",
            "Fold 2, Epoch 1, Training Loss: 1.4049662828445435\n",
            "Fold 2, Epoch 2, Training Loss: 1.3244098456700644\n",
            "Fold 2, Epoch 3, Training Loss: 1.222886315981547\n",
            "Fold 2, Epoch 4, Training Loss: 1.1568617987632752\n",
            "Fold 2, Epoch 5, Training Loss: 1.0892854142189026\n",
            "Fold 2, Epoch 6, Training Loss: 1.0329752341906229\n",
            "Fold 2, Epoch 7, Training Loss: 0.9972317393620809\n",
            "Fold 2, Epoch 8, Training Loss: 0.9508425831794739\n",
            "Fold 2, Epoch 9, Training Loss: 0.9136485306421915\n",
            "Fold 2, Epoch 10, Training Loss: 0.8733276271820068\n",
            "Fold 2, Validation Accuracy: 0.6514392991239049\n",
            "Fold 3, Epoch 1, Training Loss: 1.2579375839233398\n",
            "Fold 3, Epoch 2, Training Loss: 1.1422117471694946\n",
            "Fold 3, Epoch 3, Training Loss: 1.0445692213376363\n",
            "Fold 3, Epoch 4, Training Loss: 1.0019890809059142\n",
            "Fold 3, Epoch 5, Training Loss: 0.9196091643969218\n",
            "Fold 3, Epoch 6, Training Loss: 0.8779312173525492\n",
            "Fold 3, Epoch 7, Training Loss: 0.825063009262085\n",
            "Fold 3, Epoch 8, Training Loss: 0.7796179127693176\n",
            "Fold 3, Epoch 9, Training Loss: 0.7401275237401327\n",
            "Fold 3, Epoch 10, Training Loss: 0.7110029451052348\n",
            "Fold 3, Validation Accuracy: 0.6533166458072591\n",
            "Fold 4, Epoch 1, Training Loss: 1.1550337227185568\n",
            "Fold 4, Epoch 2, Training Loss: 1.0003560694058735\n",
            "Fold 4, Epoch 3, Training Loss: 0.9240025401115417\n",
            "Fold 4, Epoch 4, Training Loss: 0.842538644472758\n",
            "Fold 4, Epoch 5, Training Loss: 0.7980817437171936\n",
            "Fold 4, Epoch 6, Training Loss: 0.7466415087381999\n",
            "Fold 4, Epoch 7, Training Loss: 0.6807818945248921\n",
            "Fold 4, Epoch 8, Training Loss: 0.6517137002944946\n",
            "Fold 4, Epoch 9, Training Loss: 0.6245086522897084\n",
            "Fold 4, Epoch 10, Training Loss: 0.5673618920644125\n",
            "Fold 4, Validation Accuracy: 0.7384230287859824\n"
          ]
        }
      ],
      "source": [
        "# Check if multiple GPUs are available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "if torch.cuda.device_count() > 1:\n",
        "    print(\"Using\", torch.cuda.device_count(), \"GPUs for parallel training.\")\n",
        "    model = nn.DataParallel(CustomMobileNet())\n",
        "else:\n",
        "    model = CustomMobileNet()\n",
        "model.to(device)\n",
        "\n",
        "# Training loop using K-fold cross-validation\n",
        "for fold, (train_index, val_index) in enumerate(stratified_kfold.split(data_df, data_df['target'])):\n",
        "    train_subset = data_df.iloc[train_index]\n",
        "    val_subset = data_df.iloc[val_index]\n",
        "\n",
        "    # Create DataLoader instances for training and validation subsets\n",
        "    train_loader = DataLoader(CustomDataset(train_subset, '/content/drive/MyDrive/EyeDataset/Preprocessed Images/', transform=data_transforms), batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(CustomDataset(val_subset, '/content/drive/MyDrive/EyeDataset/Preprocessed Images/', transform=data_transforms), batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Add regularization techniques (e.g., dropout and batch normalization)\n",
        "    model.fc = nn.Sequential(\n",
        "        nn.Dropout(dropout_rate),  # Add dropout with the specified dropout rate\n",
        "        model.fc,\n",
        "        nn.BatchNorm1d(num_features=8).to(device)  # Add batch normalization\n",
        "    )\n",
        "\n",
        "    # Define loss function and optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for inputs, targets, keywords in train_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            targets = targets.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        # Print average training loss for each epoch\n",
        "        print(f\"Fold {fold + 1}, Epoch {epoch + 1}, Training Loss: {running_loss / len(train_loader)}\")\n",
        "\n",
        "    # Validation loop (evaluate model on validation set after training)\n",
        "    model.eval()\n",
        "    val_accuracy = 0.0\n",
        "    total_val_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets, keywords in val_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            targets = targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            val_accuracy += (predicted == targets).sum().item()\n",
        "            total_val_samples += len(targets)\n",
        "\n",
        "    # Print validation accuracy for the fold\n",
        "    print(f\"Fold {fold + 1}, Validation Accuracy: {val_accuracy / total_val_samples}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-_kVKZ7HMzkJ"
      },
      "outputs": [],
      "source": [
        "# Define your custom ResNet model\n",
        "class CustomResNet(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(CustomResNet, self).__init__()\n",
        "        self.resnet = models.resnet18(pretrained=False)  # You can choose any ResNet variant\n",
        "        num_ftrs = self.resnet.fc.in_features\n",
        "        self.resnet.fc = nn.Linear(num_ftrs, num_classes)\n",
        "\n",
        "        # Move the entire model to CUDA\n",
        "        self.to(torch.device('cuda:0'))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.resnet(x)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = CustomResNet(num_classes=8)  # Assuming 8 classes for eye disease detection\n",
        "model.to(device)\n",
        "\n",
        "# Training loop using K-fold cross-validation\n",
        "for fold, (train_index, val_index) in enumerate(stratified_kfold.split(data_df, data_df['target'])):\n",
        "    train_subset = data_df.iloc[train_index]\n",
        "    val_subset = data_df.iloc[val_index]\n",
        "\n",
        "    # Create DataLoader instances for training and validation subsets\n",
        "    train_loader = DataLoader(CustomDataset(train_subset, '/content/drive/MyDrive/EyeDataset/Preprocessed Images/', transform=data_transforms), batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(CustomDataset(val_subset, '/content/drive/MyDrive/EyeDataset/Preprocessed Images/', transform=data_transforms), batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Add regularization techniques (e.g., dropout and batch normalization)\n",
        "    model.fc = nn.Sequential(\n",
        "        nn.Dropout(dropout_rate),  # Add dropout with the specified dropout rate\n",
        "        nn.Linear(512, 8),\n",
        "        nn.BatchNorm1d(num_features=8).to(device)  # Add batch normalization\n",
        "    )\n",
        "\n",
        "    # Define loss function and optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for inputs, targets, keywords in train_loader:\n",
        "            # inputs = inputs.to(device)\n",
        "            # targets = targets.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs.to(device))\n",
        "            loss = criterion(outputs, targets.to(device))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        # Print average training loss for each epoch\n",
        "        print(f\"Fold {fold + 1}, Epoch {epoch + 1}, Training Loss: {running_loss / len(train_loader)}\")\n",
        "\n",
        "    # Validation loop (evaluate model on validation set after training)\n",
        "    model.eval()\n",
        "    val_accuracy = 0.0\n",
        "    total_val_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets, keywords in val_loader:\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            val_accuracy += (predicted == targets).sum().item()\n",
        "            total_val_samples += len(targets)\n",
        "\n",
        "    # Print validation accuracy for the fold\n",
        "    print(f\"Fold {fold + 1}, Validation Accuracy: {val_accuracy / total_val_samples}\")"
      ],
      "metadata": {
        "id": "hiiGiQ1aYNV8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "e5d55730-2467-4838-9719-a324b3a94509"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'torch' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-41a2ec96d0a8>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Check if multiple GPUs are available\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda:0\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCustomResNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch-xla\n",
        "import torch_xla.core.xla_model as xm\n",
        "import torch_xla.distributed.parallel_loader as pl"
      ],
      "metadata": {
        "id": "BTgB96Q1YhP8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if TPU is available\n",
        "device = xm.xla_device()\n",
        "# Instantiate the model and move it to the TPU device\n",
        "model = CustomResNet().to(device)\n",
        "\n",
        "# Training loop using K-fold cross-validation\n",
        "for fold, (train_index, val_index) in enumerate(stratified_kfold.split(data_df, data_df['target'])):\n",
        "    train_subset = data_df.iloc[train_index]\n",
        "    val_subset = data_df.iloc[val_index]\n",
        "\n",
        "    # Create DataLoader instances for training and validation subsets\n",
        "    train_loader = pl.MpDeviceLoader(DataLoader(CustomDataset(train_subset, '/content/drive/MyDrive/EyeDataset/Preprocessed Images/', transform=data_transforms), batch_size=batch_size, shuffle=True), device)\n",
        "    val_loader = pl.MpDeviceLoader(DataLoader(CustomDataset(val_subset, '/content/drive/MyDrive/EyeDataset/Preprocessed Images/', transform=data_transforms), batch_size=batch_size, shuffle=False), device)\n",
        "\n",
        "    # Add regularization techniques (e.g., dropout and batch normalization)\n",
        "    model.fc = nn.Sequential(\n",
        "        nn.Dropout(dropout_rate),  # Add dropout with the specified dropout rate\n",
        "        model.fc,\n",
        "        nn.BatchNorm1d(num_features=8).to(device)  # Add batch normalization\n",
        "    )\n",
        "\n",
        "    # Define loss function and optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for inputs, targets, keywords in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            xm.optimizer_step(optimizer)  # Perform optimizer step using TPU-specific function\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        # Print average training loss for each epoch\n",
        "        print(f\"Fold {fold + 1}, Epoch {epoch + 1}, Training Loss: {running_loss / len(train_loader)}\")\n",
        "\n",
        "    # Validation loop (evaluate model on validation set after training)\n",
        "    model.eval()\n",
        "    val_accuracy = 0.0\n",
        "    total_val_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets, keywords in val_loader:\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            val_accuracy += (predicted == targets).sum().item()\n",
        "            total_val_samples += len(targets)\n",
        "\n",
        "    # Print validation accuracy for the fold\n",
        "    print(f\"Fold {fold + 1}, Validation Accuracy: {val_accuracy / total_val_samples}\")"
      ],
      "metadata": {
        "id": "mTpHk13cYhmd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480
        },
        "outputId": "5c2ff0c1-aa27-4ade-b6ef-028ccb67bb3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:PJRT is now the default runtime. For more information, see https://github.com/pytorch/xla/blob/master/docs/pjrt.md\n",
            "WARNING:root:Defaulting to PJRT_DEVICE=CPU\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-5beb65e0fb48>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mrunning_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeywords\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch_xla/distributed/parallel_loader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch_xla/distributed/parallel_loader.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batches_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m       \u001b[0mxm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmark_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch_xla/distributed/parallel_loader.py\u001b[0m in \u001b[0;36mnext_item\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    132\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mnext_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0mdqueue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_queues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mdqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch_xla/utils/keyd_queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m       \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_items\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_close_write\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ready_cv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m       \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_items\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopleft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_items\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rBo9tN6Qlp7N"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "JY4Ywq_OdRc0",
        "S4EU8sIlVJNC",
        "tiMIrSagNuWI",
        "HPMvn2pZBcrm",
        "N_C_bMPML9vJ"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}